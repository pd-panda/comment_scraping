# -*- coding: utf-8 -*-
"""graph1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wc2anvqF5VfZ8jBgMMN3D21SpjXbpndM
"""

#構文解析用
!pip install janome
# Word Cloudのライブラリ
!pip install wordcloud
#  絵文字を扱うためのライブラリ
!pip install emoji --upgrade
#日本語対応ライブラリー
!pip install japanize-matplotlib

import matplotlib.pyplot as plt
import pandas as pd
import datetime
import os
import shutil

# coding:utf-8
import csv
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from bs4 import BeautifulSoup
from collections import Counter, defaultdict
import japanize_matplotlib #日本語化matplotlib
#sns.set(font="IPAexGothic") #日本語フォント設定

from janome.tokenizer import Tokenizer



#csvファイルを直下ファイルにコピー
def copy_csv(path):
    files = os.listdir(path)
#path内にあるファイルすべて取得
    files_file = [f for f in files if os.path.isfile(os.path.join(path, f))]
    for row in files_file:
        if row[-3:] == 'csv': 
            shutil.copyfile(path + '/' + row, "./"+row)

#csvファイルをデータフレームに変換
def csv_df(fname):
    df = pd.read_csv(fname,encoding='cp932')
    return df

def my_Ture(row):
    for i in range(len(row)) :
        if row[i] :
            return i
    return False

#文章から，単語抽出&出現量作成
def string_word_point(df):
    tmp_word =[]
    tokenizer = Tokenizer()
    df_time_word = pd.DataFrame(index=[], columns=['time','word'])
    df_word_point = pd.DataFrame(index=[], columns=['word','point'])
    df_time_point = pd.DataFrame(index=[], columns=['time','point'])    


    for i in range(len(df)):
        for token in tokenizer.tokenize(df['comment'][i]):
            tmp_word = token.surface

            

            if False != my_Ture(df_time_point['time'] == df['time'][i]) :
                df_time_point['point'][len(df_time_point)-1]+=1
            else :
                df_time_point = df_time_point.append({'time': df['time'][i], 'point': 1}, ignore_index=True)

            if 0 != word_Classification(token.part_of_speech):

            #名詞なら
                if word_Classification(token.part_of_speech) == '名詞':
                    tmp = my_Ture(tmp_word == df_word_point['word'])
                    if False != tmp :
                        df_word_point['point'][tmp] += 1
                    else :
                        df_word_point = df_word_point.append({'word':tmp_word,'point': 1}, ignore_index=True)

                    df_time_word = df_time_word.append({'time':df['time'][i],'word': tmp_word}, ignore_index=True)

    return df_time_word,df_word_point,df_time_point

#pointの多い順にソート
def rank_sort(rank):
    return rank.sort_values('point',ascending=True)

def df_rank(df):

#単語の種類とその数の取得
    result = string_word_point(df)

#ソート
    tmp=rank_sort(result[1])

    return result[0],tmp,result[2]

def make_rank():

        path = '.'
        files = os.listdir(path)
    #path内にあるファイルすべて取得
        files_file = [f for f in files if os.path.isfile(os.path.join(path, f))]
        for row in files_file:
            if row[-3:] == 'csv': 
                df = csv_df(row)
                result = df_rank(df)
                print(result[0].head())
                glaph_word_point(result[1])
                print(result[2].head())

def glaph_word_point(df):
    plt.tight_layout()
    plt.rcParams["font.size"] = 25
    plt.figure(figsize=(10,20 ), dpi=50,facecolor='#FFFFFF')
    plt.barh(df['word'], df['point'])
    plt.grid(which='major',color='black',linestyle='-',axis = "x")

def main(path_in,path_out) :
    #csvファイルを直下にコピー
        copy_csv(path_in)

        make_rank()

def  word_Classification(tmp):
    l = tmp.split(',')

    if l[0] == '名詞':
        if l[1] == '接尾':
            return 0
        else : return'名詞'

    else :return 0

main('/content/drive/My Drive/panda','/content/drive/My Drive/panda')

